Practical 07
AIM:- POS tagging: Viterbi Decoding & Building POS Tagger

import nltk
import sys
from nltk.corpus import brown

import nltk
nltk.download('brown')

brown_tags_words = [ ]
for sent in brown.tagged_sents():
    # sent is a list of word/tag pairs
    # add START/START at the beginning
    brown_tags_words.append( ("START", "START") )
    # then all the tag/word pairs for the word/tag pairs in the sentence.
    # shorten tags to 2 characters each
    brown_tags_words.extend([ (tag[:2], word) for (word, tag) in sent ])
    # then END/END
    brown_tags_words.append( ("END", "END") )

# conditional frequency distribution
cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)
# conditional probability distribution
cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)
print("The probability of an adjective (JJ) being 'new' is", cpd_tagwords["JJ"].prob("new"))
print("The probability of a verb (VB) being 'duck' is", cpd_tagwords["VB"].prob("duck"))

brown_tags = [tag for (tag, word) in brown_tags_words ]
# make conditional frequency distribution:
# count(t{i-1} ti)
cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))
# make conditional probability distribution, using
# maximum likelihood estimate:
# P(ti | t{i-1})
cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)
print("If we have just seen 'DT', the probability of 'NN' is", cpd_tags["DT"].prob("NN"))
print( "If we have just seen 'VB', the probability of 'JJ' is", cpd_tags["VB"].prob("DT"))
print( "If we have just seen 'VB', the probability of 'NN' is", cpd_tags["VB"].prob("NN"))


prob_tagsequence = cpd_tags["START"].prob("PP") * cpd_tagwords["PP"].prob("I") * \
cpd_tags["PP"].prob("VB") * cpd_tagwords["VB"].prob("want") * \
cpd_tags["VB"].prob("TO") * cpd_tagwords["TO"].prob("to") * \
cpd_tags["TO"].prob("VB") * cpd_tagwords["VB"].prob("race") * \
cpd_tags["VB"].prob("END")
print( "The probability of the tag sequence 'START PP VB TO VB END' for 'I want to race' is:",
prob_tagsequence)


prob_tagsequence = cpd_tags["START"].prob("PP") * cpd_tagwords["PP"].prob("I") * \
cpd_tags["PP"].prob("VB") * cpd_tagwords["VB"].prob("saw") * \
cpd_tags["VB"].prob("PP") * cpd_tagwords["PP"].prob("her") * \
cpd_tags["PP"].prob("NN") * cpd_tagwords["NN"].prob("duck") * \
cpd_tags["NN"].prob("END")
print( "The probability of the tag sequence 'START PP VB PP NN END' for 'I saw her duck' is:",
prob_tagsequence)


prob_tagsequence = cpd_tags["START"].prob("PP") * cpd_tagwords["PP"].prob("I") * \
cpd_tags["PP"].prob("VB") * cpd_tagwords["VB"].prob("saw") * \
cpd_tags["VB"].prob("PP") * cpd_tagwords["PP"].prob("her") * \
cpd_tags["PP"].prob("VB") * cpd_tagwords["VB"].prob("duck") * \
cpd_tags["VB"].prob("END")
print( "The probability of the tag sequence 'START PP VB PP VB END' for 'I saw her duck' is:",
prob_tagsequence)


prob_tagsequence = cpd_tags["START"].prob("PP") * cpd_tagwords["PP"].prob("I") * \
cpd_tags["PP"].prob("VB") * cpd_tagwords["VB"].prob("saw") * \
cpd_tags["VB"].prob("PP") * cpd_tagwords["PP"].prob("her") * \
cpd_tags["PP"].prob("VB") * cpd_tagwords["VB"].prob("duck") * \
cpd_tags["VB"].prob("END")
print( "The probability of the tag sequence 'START PP VB PP VB END' for 'I saw her duck' is:",
prob_tagsequence)

distinct_tags = set(brown_tags)
sentence = ["I", "want", "to", "race" ]
#sentence = ["I", "saw", "her", "duck" ]
sentlen = len(sentence)


viterbi = []
# backpointer:
backpointer = []
first_viterbi = {}
first_backpointer = {}

for tag in distinct_tags:
    # don't record anything for the START tag
    if tag == "START": 
        continue
    first_viterbi[tag] = cpd_tags["START"].prob(tag) * cpd_tagwords[tag].prob(sentence[0])
    first_backpointer[tag] = "START"

print(first_viterbi)
print(first_backpointer)
viterbi.append(first_viterbi)
backpointer.append(first_backpointer)
currbest = max(first_viterbi.keys(), key=lambda tag: first_viterbi[tag])
print("Word", "'" + sentence[0] + "'", "current best two-tag sequence:", first_backpointer[currbest], currbest)
# print( "Word", "'" + sentence[0] + "'", "current best tag:", currbest)

for wordindex in range(1, len(sentence)):
    this_viterbi = {}
    this_backpointer = {}
    prev_viterbi = viterbi[-1]
    for tag in distinct_tags:
        # don't record anything for the START tag
        if tag == "START":
            continue
        best_previous = max(prev_viterbi.keys(), key=lambda prevtag: \
            prev_viterbi[prevtag] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))
        this_viterbi[tag] = prev_viterbi[best_previous] * \
            cpd_tags[best_previous].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex])
        this_backpointer[tag] = best_previous
    currbest = max(this_viterbi.keys(), key=lambda tag: this_viterbi[tag])
    print("Word", "'" + sentence[wordindex] + "'", "current best two-tag sequence:", this_backpointer[currbest], currbest)
    viterbi.append(this_viterbi)
    backpointer.append(this_backpointer)

prev_viterbi = viterbi[-1]
best_previous = max(prev_viterbi.keys(), key=lambda prevtag: prev_viterbi[prevtag] * cpd_tags[prevtag].prob("END"))
prob_tagsequence = prev_viterbi[best_previous] * cpd_tags[best_previous].prob("END")
# best tagsequence: we store this in reverse for now, will invert later
best_tagsequence = ["END", best_previous]
# invert the list of backpointers
backpointer.reverse()
current_best_tag = best_previous

for bp in backpointer:
    best_tagsequence.append(bp[current_best_tag])
    current_best_tag = bp[current_best_tag]

best_tagsequence.reverse()
print("The sentence was:", end=" ")
for w in sentence: 
    print(w, end=" ")
print("\n")
print("The best tag sequence is:", end=" ")
for t in best_tagsequence: 
    print(t, end=" ")
print("\n")
print("The probability of the best tag sequence is:", prob_tagsequence)






