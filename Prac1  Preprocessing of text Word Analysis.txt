Prac 1  Preprocessing of text: Word Analysis (Tokenization, Filtration, word frequency, Stop Word
Removal)

import nltk
nltk.download('punkt')
#sentence token
from nltk.tokenize import sent_tokenize
text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard"""
tokenized_text=sent_tokenize(text)
print(tokenized_text)

#word token
from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)

#freq dist
from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word)
print(fdist)

# Frequency Distribution Plot
import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()

nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)

#removing stopword
filtered_sent=[]
for w in tokenized_word:
    if w not in stop_words:
         filtered_sent.append(w)
print("Tokenized Word:",tokenized_word)
print("Filterd Word:",filtered_sent)
